{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticating(credential):\n",
    "    logging('\\n\\nfunction>>>>>authenticating')\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(credential[\"api_key\"], credential[\"api_secret\"])\n",
    "    auth.set_access_token(credential[\"access_token\"], credential[\"access_token_secret\"])\n",
    "    \n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_retweet_tweet(api,tweet,dict_tweets_info):\n",
    "    logging('\\n\\nfunction>>>>>print_and_retweet_tweet')\n",
    "    \n",
    "    try: \n",
    "        logging('appending infos to dictionary')\n",
    "        dict_tweets_info['created_at'].append(str(tweet.created_at))\n",
    "        dict_tweets_info['tweet_ID'].append(str(tweet.id))\n",
    "        dict_tweets_info['user'].append(str(tweet.user.screen_name))\n",
    "        dict_tweets_info['tweet_content'].append((tweet.text))\n",
    "        dict_tweets_info['place'].append(str(tweet.place))\n",
    "        dict_tweets_info['language'].append(str(tweet.lang))\n",
    "        dict_tweets_info['source'].append(str(tweet.source_url).replace(\"http://twitter.com/download/\",\"\"))\n",
    "    \n",
    "        logging('----------------------------------------')\n",
    "        logging('collected informations')\n",
    "        logging('----------------------------------------')\n",
    " \n",
    "        logging('dict_tweets_info: \\n '+str(dict_tweets_info))\n",
    "        logging('----------------------------------------\\n\\n\\n\\n')\n",
    "        \n",
    "        # ----- starting filters ------\n",
    "        \n",
    "        logging('print_and_retweet_tweet(): better filtering BEFORE retweet')\n",
    "        string_tweet_content = \"\".join(dict_tweets_info['tweet_content'] )\n",
    "        if not 'zolpidem' in string_tweet_content.lower():\n",
    "            logging('NAO ACHOU ZOLPIDEM NA STRING')\n",
    "            # NO WAY it's gonna retweet something that has NOT 'zolpidem on it'\n",
    "            return False\n",
    "\n",
    "        string_lang_content = \"\".join(dict_tweets_info['language'] )\n",
    "        logging('STRING_LANG_CONTENT: '+ string_lang_content )\n",
    "\n",
    "        if string_lang_content in ['ja','ko']:\n",
    "            logging('ta em japones ou coreano')\n",
    "            # NO WAY it's gonna retweet something that is in japanese or korean\n",
    "            return False\n",
    "        else:\n",
    "            logging('teoricamente nao esta em japones nem coreano')\n",
    "    \n",
    "        logging('retweeting ←←←←←←')\n",
    "        api.retweet(tweet.id)\n",
    "        logging('→→→→→→→ retweeted')\n",
    "        return dict_tweets_info\n",
    "    \n",
    "    except tweepy.TweepError as e: \n",
    "        if e.api_code == 327:\n",
    "            logging('You have already retweeted this Tweet')\n",
    "            return False\n",
    "        \n",
    "    except tweepy.RateLimitError as e:\n",
    "        logging('Excedeu limite por tempo?')\n",
    "        logging('erro: '+str(e))\n",
    "        logging('DORMINDO POR QUINZE MINUTOS')\n",
    "        time.sleep(60 * 15)  # we supposedly saw a rate limit that is ignored after 15 min ??? so we should wait 15 min to retry \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_and_updates_value(path,incrementa_contagem_de_falha=False,inicializar = False):\n",
    "    logging('\\n\\nfunction>>>>>write_json_and_updates_value')\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_date = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # try to read from file\n",
    "    try:\n",
    "        with open(path) as json_file:\n",
    "            tweets_status = json.load(json_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # write on file\n",
    "    # if our current date is the same, increase amount of tweets.\n",
    "    # if our current date is different, amount is ZERO !!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    if inicializar or tweets_status['current_date'] != current_date: \n",
    "        logging('different dates, OR initializing, so we need to change the current_date value and also turn into 0 all the values')\n",
    "        with open(path, 'w') as f:\n",
    "            try:\n",
    "                content = {\"current_date\": current_date,\n",
    "                           \"amount_of_tweets\": 0,\n",
    "                           \"total_amount_including_failure\":0}\n",
    "                json.dump(content, f)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                logging('decode error but will try raw writing')\n",
    "                f.write(contenting)\n",
    "    else: \n",
    "        logging('same date!! so, just change the value of tweetts')\n",
    "        if not incrementa_contagem_de_falha:\n",
    "                logging('increases both keys , the including failure and the sucessed amounts')\n",
    "                #vai incrementtar o total com falhas tb + o total dos sucessos\n",
    "                tweets_status[\"amount_of_tweets\"] = tweets_status[\"amount_of_tweets\"]+1 \n",
    "                tweets_status['total_amount_including_failure'] = tweets_status['total_amount_including_failure']+1\n",
    "                with open(path, 'w') as f:\n",
    "                    try:\n",
    "                        json.dump(tweets_status, f)\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging('decode error but will try raw writing')\n",
    "                        f.write(contenting)\n",
    "                    \n",
    "        elif incrementa_contagem_de_falha:\n",
    "                # vai incrementar SOMENTE chave com total de tweets, independente de ter falhado ou nao\n",
    "                logging('INCREMENTANDO CHAVE DE CONTAGEM TOTAL DE TWEETS')\n",
    "                     # increasing amount of the ones who failure \n",
    "                tweets_status['total_amount_including_failure'] = tweets_status['total_amount_including_failure']+1\n",
    "\n",
    "                with open(path, 'w') as f:\n",
    "                    try:\n",
    "                        json.dump(tweets_status, f)\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging('decode error but will try raw writing')\n",
    "                        f.write(contenting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_infos_to_csv(valid_tweet):\n",
    "    logging('\\n\\nfunction>>>>>exporting_infos_to_csv')\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_directory = os.getcwd()  \n",
    "    timestamp = now.strftime(\"%d/%m/%Y\").replace(\"/\",\"-\").replace(':',\"-\").replace(',','--').replace(\" \",\"\")\n",
    "\n",
    "    CSV_path = current_directory+'\\\\arquivos_bot\\\\dados_exportados\\\\dados_'+timestamp+'.csv'\n",
    "    logging('log path: '+str(CSV_path))\n",
    "\n",
    "    logging('dict_tweets_info : '+str(valid_tweet))\n",
    "\n",
    "    # pegando os valores do dicionario e jogando em lista pq senao ele da apend no dicionario inteiro, linha de key e depois linha de value PRA CADA tweet\n",
    "    lista_valores_atuais = []\n",
    "    for key, value in valid_tweet.items():\n",
    "        lista_valores_atuais.append(\"\".join(value))\n",
    "\n",
    "    # forcing Tweet ID to be written as string, so it doesnt truncate as scientific notation\n",
    "    lista_valores_atuais[1] = '\\''+lista_valores_atuais[1]\n",
    "\n",
    "    logging('LISTA_VALORES_ATUAIS: '+str(lista_valores_atuais))\n",
    "\n",
    "    # se arquivo do dia já existe, vai dar append apenas no conteúdo daquele tweet, caso contrário, \n",
    "    # vai criar o arquivo e vai dar append\n",
    "    # no header e depois no tweet \n",
    "\n",
    "    if not os.path.exists(CSV_path):\n",
    "        logging('today s csv does not exist yet, creating it and appending header')\n",
    "        header_csv = ['created_at','tweet_ID','user','tweet_content','place','language','source'] \n",
    "        with open(CSV_path, \"a\") as file:\n",
    "            wr = csv.writer(file)\n",
    "            wr.writerow(header_csv)\n",
    "            \n",
    "    with open(CSV_path, \"a\",encoding=\"utf-8\") as file:\n",
    "        logging('writing lista_valores_atuais anyways')\n",
    "        wr = csv.writer(file)\n",
    "        wr.writerow(lista_valores_atuais)\n",
    "\n",
    "    # df = pd.DataFrame(lista_valores_atuais) # turning into data frame\n",
    "    # df.to_csv(path_or_buf = CSV_path, mode='a',index=False, cols = header_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(text_to_log=\"\"):\n",
    "    \n",
    "    # converte parâmetro que ele quer escrever no log em string, caso tenha sido enviado em outro formato\n",
    "    text_to_log = str(text_to_log)\n",
    "    \n",
    "    # prepara data e timestamp pra dar append no arquivo de log (ou escrever um novo) + timestamp no conteúdo do arquivo\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%d/%m/%Y\").replace(\"/\",\"-\")\n",
    "    timestamp = now.strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "    \n",
    "    # busca diretório onde o robô está rodando e formata o path pro log do dia corrente\n",
    "    current_directory = os.getcwd()  \n",
    "    log_path = current_directory+'\\\\arquivos_bot\\\\logs\\\\log_'+date+'.txt'\n",
    "\n",
    "    # se arquivo do dia já existir, só escreve nele o timestamp + conteúdo do parámetro\n",
    "    # caso contrário, cria o arquivo de log daquele dia \n",
    "    with open(log_path, 'a+',encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(timestamp+ ' - ' + text_to_log+'\\n')\n",
    "    \n",
    "    # além de escrever no arquivo, printa no jupyter\n",
    "    print(timestamp+ ' - ' + text_to_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_special_text_to_ascii(original_text):\n",
    "    translated_text = ''\n",
    "\n",
    "    for character in original_text:\n",
    "        if ord(character) >= 128:\n",
    "            translated_text = translated_text + '\"Chr(' + str(ord(character)) + ')\"'\n",
    "        else:\n",
    "            translated_text = translated_text + character\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # cria pastas que o robô usa caso não existam \n",
    "    current_directory = os.getcwd()\n",
    "    if not os.path.exists(current_directory+'\\\\arquivos_bot\\\\logs'):\n",
    "        pymsgbox.alert(text='Criando pasta de logs', title='Preparando bot', button='OK',timeout=4500)\n",
    "        os.makedirs(current_directory+'\\\\arquivos_bot\\\\logs')\n",
    "        logging(\"Preparando ambiente\")\n",
    "        logging(\"Criando pasta de logs\")\n",
    "    \n",
    "    if not os.path.exists(current_directory+'\\\\arquivos_bot\\\\controle'):\n",
    "        pymsgbox.alert(text='Criando pasta de controle', title='Preparando bot', button='OK',timeout=4500)\n",
    "        os.makedirs(current_directory+'\\\\arquivos_bot\\\\controle')\n",
    "        logging(\"Criando pasta de controle\")\n",
    "\n",
    "    if not os.path.exists(current_directory+'\\\\arquivos_bot\\\\dados_exportados'):\n",
    "        pymsgbox.alert(text='Criando pasta de dados exportados', title='Preparando bot', button='OK',timeout=4500)\n",
    "        os.makedirs(current_directory+'\\\\arquivos_bot\\\\dados_exportados')\n",
    "        logging(\"Criando pasta de dados exportados\")\n",
    "    \n",
    "    pymsgbox.alert('Pastas necessárias para o robô conferidas, iniciando o bot', 'Starting bot',timeout=5000)\n",
    "    logging(\" --------- INICIANDO ROBÔ ---------\")\n",
    "    \n",
    "    # checking if control json exists, otherwise we create it\n",
    "    if not os.path.exists(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json'):\n",
    "        logging(\"arquivo json não encontrado\")\n",
    "        now = datetime.now()\n",
    "        date = now.strftime(\"%d/%m/%Y\")\n",
    "        write_json_and_updates_value(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json',incrementa_contagem_de_falha=False,inicializar = True)\n",
    "    \n",
    "    credential =  {\n",
    "                    \"api_key\" : credentials.API_KEY,\n",
    "                    \"api_secret\" : credentials.API_SECRET,\n",
    "                    \"bearer_token\" : credentials.BEARER_TOKEN,\n",
    "                    \"access_token\" : credentials.ACCESS_TOKEN,\n",
    "                    \"access_token_secret\" : credentials.ACCESS_TOKEN_SECRET\n",
    "                    }\n",
    "    \n",
    "    api = authenticating(credential)\n",
    "    \n",
    "    \n",
    "    for tweet in tweepy.Cursor(api.search, q='zolpidem').items(1100):\n",
    "\n",
    "        dict_tweets_info = {\n",
    "        \"created_at\": [],\n",
    "        \"tweet_ID\": [],\n",
    "        \"user\": [],\n",
    "        \"tweet_content\": [],\n",
    "        \"place\": [],\n",
    "        \"language\": [],\n",
    "        \"source\": [] \n",
    "    }\n",
    "        \n",
    "        with open(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json') as json_file:\n",
    "            tweets_status = json.load(json_file)\n",
    "            if tweets_status[\"amount_of_tweets\"] == 999 and tweets_status == date:\n",
    "                sys.exit('DAILY LIMIT REACHED, CANT RETWEET MORE THAN 1000 TWEETS')\n",
    "                \n",
    "        valid_tweet = print_and_retweet_tweet(api,tweet,dict_tweets_info)\n",
    "        \n",
    "        if not valid_tweet:\n",
    "            logging('Tweet is not valid for some reason thus we retrieve another one')\n",
    "            write_json_and_updates_value(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json',incrementa_contagem_de_falha=True)\n",
    "            continue\n",
    "            \n",
    "        if isinstance(valid_tweet,dict):\n",
    "            logging('Ok, we received a dict as return, we may export the results now')\n",
    "            export_infos_to_csv(valid_tweet)\n",
    "            write_json_and_updates_value(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json',incrementa_contagem_de_falha=False)\n",
    "        else:\n",
    "            logging('Unexpected return for print_and_retweet_tweet different than dict or false!! content: '+str(valid_tweet))\n",
    "            write_json_and_updates_value(current_directory+'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json',incrementa_contagem_de_falha=False)\n",
    "        logging(\"Waiting 2 min for retrieve another tweet cuz we like safety\")\n",
    "        time.sleep(60*2) # sleep 2 min, so we dont reach the limit 100 tweets per hour\n",
    "        \n",
    "    logging('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ FIM DA LAP $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ ')   \n",
    "    pymsgbox.alert('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ \\n         FIM DA LAP\\n $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$', 'End of times ',timeout=40000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/01/2021, 21:31:20 -  --------- INICIANDO ROBÔ ---------\n",
      "04/01/2021, 21:31:20 - \n",
      "\n",
      "function>>>>>authenticating\n",
      "04/01/2021, 21:31:20 - \n",
      "\n",
      "function>>>>>print_and_retweet_tweet\n",
      "04/01/2021, 21:31:20 - appending infos to dictionary\n",
      "04/01/2021, 21:31:20 - ----------------------------------------\n",
      "04/01/2021, 21:31:20 - collected informations\n",
      "04/01/2021, 21:31:20 - ----------------------------------------\n",
      "04/01/2021, 21:31:20 - dict_tweets_info: \n",
      " {'created_at': ['2021-01-05 00:29:39'], 'tweet_ID': ['1346252484940075020'], 'user': ['kkkblz'], 'tweet_content': ['será que posso misturar vodka c zolpidem'], 'place': ['None'], 'language': ['pt'], 'source': ['iphone']}\n",
      "04/01/2021, 21:31:20 - ----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<class 'list'>\n",
      "04/01/2021, 21:31:20 - retweeting ←←←←←←\n",
      "04/01/2021, 21:31:21 - →→→→→→→ retweeted\n",
      "04/01/2021, 21:31:21 - Ok, we received a dict as return, we may export the results now\n",
      "04/01/2021, 21:31:21 - \n",
      "\n",
      "function>>>>>exporting_infos_to_csv\n",
      "04/01/2021, 21:31:21 - log path: C:\\Users\\gabri\\Documents\\Retweet-Bot\\arquivos_bot\\dados_exportados\\dados_04-01-2021.csv\n",
      "04/01/2021, 21:31:21 - dict_tweets_info {'created_at': ['2021-01-05 00:29:39'], 'tweet_ID': ['1346252484940075020'], 'user': ['kkkblz'], 'tweet_content': ['será que posso misturar vodka c zolpidem'], 'place': ['None'], 'language': ['pt'], 'source': ['iphone']}\n",
      "04/01/2021, 21:31:21 - \n",
      "\n",
      "function>>>>>write_json_and_updates_value\n",
      "04/01/2021, 21:31:21 - same date!! so, just change the value of tweetts\n",
      "04/01/2021, 21:31:21 - increases both keys , the including failure and the sucessed amounts\n",
      "04/01/2021, 21:31:21 - Waiting 2 min for retrieve another tweet cuz we like safety\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-02bc7be78404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-f3181c4d2f4f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mwrite_json_and_updates_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_directory\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\arquivos_bot\\\\controle\\\\amount_of_tweets_from_today.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mincrementa_contagem_de_falha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Waiting 2 min for retrieve another tweet cuz we like safety\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sleep 2 min, so we dont reach the limit 100 tweets per hour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ FIM DA LAP $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import credentials\n",
    "import tweepy\n",
    "import time\n",
    "from datetime import date, datetime \n",
    "import os\n",
    "import pymsgbox \n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
